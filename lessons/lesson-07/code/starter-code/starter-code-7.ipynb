{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 7- Starter code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Residual Error (15 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Values: [  9.15540205e-26]\n",
      "Coefficients: [ 0.00096395]\n",
      "y-intercept: 0.0859173102936\n",
      "R-Squared: 0.871949198087\n",
      "Mean squared error: 0.119901525171\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEGxJREFUeJzt3XtM1fUfx/HXkQOFXJLyrLk5zMxyypqp09pS1y8J91OH\nKYhQhybOlbOUMgIZoQ3nZc5asvC2dVmumflHP3WtVuRiU8e6asKsrZRNcw0LBhxNEL6/P+rHLwLO\n4XIuvU/Px9YW58s55/05H/bkdDjfk8txHEcAAFNGRHoAAMDgEW8AMIh4A4BBxBsADCLeAGCQOxx3\n0tjYGo67CbqUlJFqaroa6TGCLhrXFY1rkqJzXaxp4DyepH6P8czbD7c7JtIjhEQ0risa1yRF57pY\nU3AQbwAwiHgDgEHEGwAMIt4AYBDxBgCDiDcAGBTwfd6dnZ0qKyvT+fPn5XK59NJLL+mmm25SSUmJ\nXC6XJk6cqI0bN2rECH4PAEC4BIz38ePHJUkHDx5UbW2tXnnlFTmOo8LCQs2aNUvl5eWqrq5Wenp6\nyIcFAPwu4NPlefPmqaKiQpL0008/KTk5WXV1dZo5c6Ykac6cOTp58mRopwQA9DCg0+PdbreKi4v1\n8ccfa9euXTpx4oRcLpckKSEhQa2t/k9/T0kZafasKn+npy5a/58wTjJwR3dmBvwef+uyKhrXJEXn\nuljT8A34s022b9+u559/XsuWLdP169e7L/f5fEpOTvZ7XaufY+DxJJn8XJZAM1tdlz/RuCYpOtfF\nmgZ3u/0J+LLJ+++/r71790qS4uPj5XK5lJaWptraWklSTU2NZsyYEaRRAQADEfCZ9yOPPKINGzbo\nscce040bN1RaWqoJEyboxRdf1Msvv6w777xTGRkZ4ZgVAPCHgPEeOXKkXn311V6XHzhwICQDAQAC\n483ZAGAQ8QYAg4g3ABhEvAHAIOINAAYRbwAwiHgDgEHEGwAMIt4AYBDxBgCDiDcAGES8AcAg4g0A\nBhFvADCIeAOAQcQbAAwi3gBgEPEGAIOINwAYRLwBwCDiDQAGEW8AMIh4A4BBxBsADCLeAGAQ8QYA\ng4g3ABjk9newo6NDpaWlunTpktrb27V69WqNGTNGTz75pO644w5JUm5urv7973+HY1YAwB/8xvvI\nkSMaNWqUduzYoebmZi1evFhr1qzRihUrVFBQEK4ZAQB/4XIcx+nvoM/nk+M4SkxMVFNTk7KysvTg\ngw/q/Pnz6uzs1Lhx41RaWqrExES/d3LjRqfc7pigDx9pi9b/J9Ij9OnozsxIjwAgxPzG+3/a2tq0\nevVqLVu2TO3t7brnnnuUlpam3bt3q6WlRcXFxX6v39jYGrSBw8njSfI7e8G2T8M4zcC9XvIvv8cD\nrcuiaFyTFJ3rYk2Du93+BPyD5eXLl5Wfn6/MzEwtWrRI6enpSktLkySlp6ervr4+eJMCAAbEb7yv\nXLmigoICFRUVKSsrS5K0cuVKnTlzRpJ06tQpTZkyJfRTAgB68PsHyz179qilpUVVVVWqqqqSJJWU\nlGjLli2KjY3V6NGjVVFREZZBAQD/5zfeZWVlKisr63X5wYMHQzYQACAwTtIBAIOINwAYRLwBwCDi\nDQAGEW8AMIh4A4BBxBsADCLeAGAQ8QYAg4g3ABhEvAHAIOINAAYRbwAwiHgDgEHEGwAM8vt53rDJ\n6v9bE8DA8cwbAAwi3gBgEPEGAIOINwAYRLwBwCDiDQAGEW8AMIh4A4BBxBsADCLeAGCQ39PjOzo6\nVFpaqkuXLqm9vV2rV6/WXXfdpZKSErlcLk2cOFEbN27UiBH8DgCAcPIb7yNHjmjUqFHasWOHmpub\ntXjxYk2aNEmFhYWaNWuWysvLVV1drfT09HDNCwBQgJdN5s+fr3Xr1kmSHMdRTEyM6urqNHPmTEnS\nnDlzdPLkydBPCQDowe8z74SEBElSW1ub1q5dq8LCQm3fvl0ul6v7eGtra8A7SUkZKbc7Jgjjhp/H\nkxTpEaJGqB/LaN2raFwXaxq+gB8Je/nyZa1Zs0Z5eXlatGiRduzY0X3M5/MpOTk54J00NV0d3pQR\n4vEkqbEx8C8nDEwoH8to3atoXBdrGtzt9sfvyyZXrlxRQUGBioqKlJWVJUmaPHmyamtrJUk1NTWa\nMWNGEEcFAAyE33jv2bNHLS0tqqqqktfrldfrVWFhoSorK5WTk6OOjg5lZGSEa1YAwB/8vmxSVlam\nsrKyXpcfOHAgZAMBAALjDdoAYBDxBgCDiDcAGES8AcAg4g0ABhFvADCIeAOAQcQbAAwi3gBgEPEG\nAIOINwAYRLwBwCDiDQAGEW8AMIh4A4BBxBsADCLeAGAQ8QYAg4g3ABhEvAHAIOINAAYRbwAwiHgD\ngEHEGwAMIt4AYBDxBgCDiDcAGDSgeJ8+fVper1eSVF9fr9mzZ8vr9crr9eqDDz4I6YAAgN7cgb5h\n//79OnLkiOLj4yVJdXV1WrFihQoKCkI+HACgbwHjnZqaqsrKSr3wwguSpLNnz+r8+fOqrq7WuHHj\nVFpaqsTERL+3kZIyUm53THAmDjOPJynSI0SNUD+W0bpX0bgu1jR8AeOdkZGhixcvdn997733Kjs7\nW2lpadq9e7dee+01FRcX+72Npqarw580AjyeJDU2tkZ6jKgRyscyWvcqGtfFmgZ3u/0Z9B8s09PT\nlZaW1v3v9fX1Q58MADAkg473ypUrdebMGUnSqVOnNGXKlKAPBQDwL+DLJn+1adMmVVRUKDY2VqNH\nj1ZFRUUo5gIA+DGgeI8dO1aHDh2SJE2ZMkUHDx4M6VAAAP84SQcADCLeAGAQ8QYAg4g3ABhEvAHA\nIOINAAYRbwAwiHgDgEHEGwAMIt4AYBDxBgCDiDcAGES8AcAg4g0ABhFvADCIeAOAQcQbAAwi3gBg\nEPEGAIOINwAYRLwBwCDiDQAGEW8AMIh4A4BBxBsADCLeAGDQgOJ9+vRpeb1eSVJDQ4Nyc3OVl5en\njRs3qqurK6QDAgB6Cxjv/fv3q6ysTNevX5ckbd26VYWFhXrnnXfkOI6qq6tDPiQAoKeA8U5NTVVl\nZWX313V1dZo5c6Ykac6cOTp58mTopgMA9Mkd6BsyMjJ08eLF7q8dx5HL5ZIkJSQkqLW1NeCdpKSM\nlNsdM4wxI8fjSYr0CFEj1I9ltO5VNK6LNQ1fwHj/1YgR/3+y7vP5lJycHPA6TU1XB3s3fwseT5Ia\nGwP/csLAhPKxjNa9isZ1sabB3W5/Bv1uk8mTJ6u2tlaSVFNToxkzZgx9MgDAkAw63sXFxaqsrFRO\nTo46OjqUkZERirkAAH4M6GWTsWPH6tChQ5Kk8ePH68CBAyEdCgDgHyfpAIBBxBsADCLeAGAQ8QYA\ng4g3ABhEvAHAIOINAAYRbwAwiHgDgEHEGwAMIt4AYBDxBgCDiDcAGES8AcAg4g0ABhFvADCIeAOA\nQcQbAAwi3gBgEPEGAIOINwAYRLwBwCDiDQAGEW8AMIh4A4BBxBsADCLeAGCQe6hXfPTRR5WYmChJ\nGjt2rLZu3Rq0oQAA/g0p3tevX5fjOHr77beDPQ8AYACGFO9z587p2rVrKigo0I0bN/Tcc89p6tSp\n/X5/SspIud0xQx4ykjyepEiPEDVC/VhG615F47pY0/ANKd4333yzVq5cqezsbF24cEGrVq3Shx9+\nKLe775traro6rCEjxeNJUmNja6THiBqhfCyjda+icV2saXC3258hxXv8+PEaN26cXC6Xxo8fr1Gj\nRqmxsVFjxowZ8pAAgIEb0rtNDh8+rG3btkmSfv75Z7W1tcnj8QR1MABA/4b0zDsrK0sbNmxQbm6u\nXC6XtmzZ0u9LJgCA4BtScePi4rRz585gzwIAGCBO0gEAg4g3ABhEvAHAIOINAAYRbwAwiHgDgEHE\nGwAMIt4AYBDxBgCDiDcAGES8AcAg4g0ABhFvADCIeAOAQcQbAAwi3gBgEPEGAIOINwAYRLwBwCDi\nDQAGEW8AMIh4A4BBLsdxnFDfSWNj67CuX7Dt0yBNAvT2esm/Ij1Cn/6uP/fDfbw8nqRhN6Ev0fh4\neTxJ/R7jmTcAGES8AcAg4g0ABrmHcqWuri5t2rRJ3333neLi4rR582aNGzcu2LMBAPoxpGfen3zy\nidrb2/Xuu+9q/fr12rZtW7DnAgD4MaR4f/nll5o9e7YkaerUqTp79mxQhwIA+Dekl03a2tqUmJjY\n/XVMTIxu3Lght7vvm/P3dpeBOLozc1jXByyK5p/74TahL9H8ePVlSM+8ExMT5fP5ur/u6urqN9wA\ngOAbUrynTZummpoaSdI333yju+++O6hDAQD8G9IZlv97t8n3338vx3G0ZcsWTZgwIRTzAQD6EJbT\n4wEAwcVJOgBgEPEGAIOINwAYRLz/5LffftMzzzyjvLw8rVq1Sr/++muv79m8ebOWLFkir9crr9er\n1tbgf7RlMHR1dam8vFw5OTnyer1qaGjocfzTTz/V0qVLlZOTo0OHDkVoysELtK4333xTCxYs6N6f\nH3/8MUKTDt7p06fl9Xp7XW51r6T+12R1nzo6OlRUVKS8vDxlZWWpurq6x/Gw7pWDbq+//rqza9cu\nx3Ec59ixY05FRUWv71m+fLnzyy+/hHu0Qfvoo4+c4uJix3Ec5+uvv3aeeuqp7mPt7e3OvHnznObm\nZuf69evOkiVLnMbGxkiNOij+1uU4jrN+/Xrn22+/jcRow7Jv3z5n4cKFTnZ2do/LLe9Vf2tyHLv7\ndPjwYWfz5s2O4zhOU1OTM3fu3O5j4d4rnnn/yZ9P+58zZ45OnTrV43hXV5caGhpUXl6u5cuX6/Dh\nw5EYc0D8fYTBDz/8oNTUVN1yyy2Ki4vT9OnT9fnnn0dq1EEJ9NEMdXV12rdvn3Jzc7V3795IjDgk\nqampqqys7HW55b3qb02S3X2aP3++1q1bJ0lyHEcxMTHdx8K9V//Y0yLfe+89vfXWWz0uu+2225SU\n9PtpuwkJCb1eErl69aoef/xxrVixQp2dncrPz1daWpomTZoUtrkHyt9HGLS1tXWvU/p9rW1tbZEY\nc9ACfTTDggULlJeXp8TERD399NM6fvy4HnrooUiNO2AZGRm6ePFir8st71V/a5Ls7lNCQoKk3/dl\n7dq1Kiws7D4W7r36xz7zzs7O1rFjx3r8k5SU1H3av8/nU3Jyco/rxMfHKz8/X/Hx8UpMTNT999+v\nc+fORWL8gPx9hMFfj/l8vh4/dH9n/tblOI6eeOIJ3XrrrYqLi9PcuXNVX18fqVGDwvJe9cf6Pl2+\nfFn5+fnKzMzUokWLui8P9179Y+Pdl2nTpumzzz6TJNXU1Gj69Ok9jl+4cEG5ubnq7OxUR0eHvvrq\nK02ZMiUSowbk7yMMJkyYoIaGBjU3N6u9vV1ffPGF7rvvvkiNOij+1tXW1qaFCxfK5/PJcRzV1tYq\nLS0tUqMGheW96o/lfbpy5YoKCgpUVFSkrKysHsfCvVf/2JdN+pKbm6vi4mLl5uYqNjZWO3fulCS9\n8cYbSk1N1cMPP6zMzEwtW7ZMsbGxyszM1MSJEyM8dd/S09N14sQJLV++vPsjDI4ePaqrV68qJydH\nJSUlWrlypRzH0dKlS3X77bdHeuQBCbSuZ599Vvn5+YqLi9MDDzyguXPnRnrkIYmGvfqraNinPXv2\nqKWlRVVVVaqqqpL0+3/FX7t2Lex7xenxAGAQL5sAgEHEGwAMIt4AYBDxBgCDiDcAGES8AcAg4g0A\nBv0Xu9VUg+jHVHsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b128890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from sklearn import linear_model, metrics\n",
    "\n",
    "# read in the mammal dataset\n",
    "wd = '../dataset/msleep/'\n",
    "mammals = pd.read_csv(wd+'msleep.csv')\n",
    "mammals = mammals[mammals.brainwt.notnull()].copy()\n",
    "\n",
    "from sklearn import feature_selection, linear_model\n",
    "\n",
    "def get_linear_model_metrics(X, y, algo):\n",
    "    # get the pvalue of X given y. Ignore f-stat for now.\n",
    "    pvals = feature_selection.f_regression(X, y)[1]\n",
    "    # start with an empty linear regression object\n",
    "    # .fit() runs the linear regression function on X and y\n",
    "    algo.fit(X,y)\n",
    "    residuals = (y-algo.predict(X)).values\n",
    "\n",
    "    # print the necessary values\n",
    "    print 'P Values:', pvals\n",
    "    print 'Coefficients:', algo.coef_\n",
    "    print 'y-intercept:', algo.intercept_\n",
    "    print 'R-Squared:', algo.score(X,y)\n",
    "    print 'Mean squared error:', metrics.mean_squared_error(y, algo.predict(X))\n",
    "    \n",
    "# metrics is the module you are using. mean_squared_error is the calculation you use\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=int(np.ceil(np.sqrt(len(y)))))\n",
    "    \n",
    "    # keep the model\n",
    "    return algo\n",
    "\n",
    "\n",
    "X = mammals[['bodywt']]\n",
    "y = mammals['brainwt']\n",
    "lm = linear_model.LinearRegression()\n",
    "lm = get_linear_model_metrics(X, y, lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "#### Intro to cross validation with bike share data from last time. We will be modeling casual ridership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "wd = '../dataset/'\n",
    "bikeshare = pd.read_csv(wd + 'bikeshare.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create dummy variables and set outcome (dependent) variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = pd.get_dummies(bikeshare.weathersit, prefix='weather')  # alternative short-cut way!\n",
    "modeldata = bikeshare[['temp', 'hum']].join(weather[['weather_1', 'weather_2', 'weather_3']])\n",
    "y = bikeshare.casual \n",
    "\n",
    "# Combining dataframes using join function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross valiation with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8990,  8451, 12807, ...,  5231,  9822, 11462])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "train indices [    0     2     4 ..., 17374 17375 17376]\n",
      "test indices [    1     3    13 ..., 17366 17377 17378]\n",
      "Model 1\n",
      "MSE: 1523.46396012\n",
      "R2: 0.311827413402\n",
      "train indices [    0     1     2 ..., 17376 17377 17378]\n",
      "test indices [    7    15    41 ..., 17356 17360 17370]\n",
      "Model 2\n",
      "MSE: 1729.93516473\n",
      "R2: 0.311888774795\n",
      "train indices [    0     1     2 ..., 17376 17377 17378]\n",
      "test indices [    9    16    18 ..., 17372 17373 17375]\n",
      "Model 3\n",
      "MSE: 1614.85146995\n",
      "R2: 0.311906876229\n",
      "train indices [    0     1     2 ..., 17375 17377 17378]\n",
      "test indices [    4     8    11 ..., 17342 17348 17376]\n",
      "Model 4\n",
      "MSE: 1744.69907561\n",
      "R2: 0.311914688455\n",
      "train indices [    1     3     4 ..., 17376 17377 17378]\n",
      "test indices [    0     2     5 ..., 17363 17371 17374]\n",
      "Model 5\n",
      "MSE: 1756.63782955\n",
      "R2: 0.311831396124\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of MSE for all folds: 1673.91749999\n",
      "Mean of R2 for all folds: 0.311873829801\n"
     ]
    }
   ],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print \"~~~~ CROSS VALIDATION each fold ~~~~\"\n",
    "for train_index, test_index in kf:\n",
    "    print \"train indices\", train_index\n",
    "    print \"test indices\", test_index\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print 'Model', n\n",
    "    print 'MSE:', mse_values[n-1]\n",
    "    print 'R2:', scores[n-1]\n",
    "    \n",
    "\n",
    "\n",
    "print \"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\"\n",
    "print 'Mean of MSE for all folds:', np.mean(mse_values)\n",
    "print 'Mean of R2 for all folds:', np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ Single Model ~~~~\n",
      "MSE of single model: 1672.58110765\n",
      "R2:  0.311934605989\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression().fit(modeldata, y)\n",
    "print \"~~~~ Single Model ~~~~\"\n",
    "print 'MSE of single model:', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'R2: ', lm.score(modeldata, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1673.9672373986514, 1673.2240959112701, 1673.0827992262721, 1673.2596034908115, 1673.4394450339482, 1673.4557176051594, 1673.2050430004431, 1673.4632468626687, 1673.439505387531, 1673.5750635587992, 1673.3284390397143, 1673.616090798663, 1673.4102293176516, 1673.6303131039763, 1673.3610285623326, 1673.1378822270385, 1673.5088447149888, 1673.5695673760479, 1673.3174176329383, 1673.432253751585, 1673.3838457145819, 1673.3360974970062, 1673.2753392866441, 1673.7038735539743, 1673.5616222475119]\n",
      "[0.31174442099389671, 0.31189672740738883, 0.31191585516600179, 0.31191576517510156, 0.31191605113615689, 0.31191918483242115, 0.31192523723548593, 0.31192269686662433, 0.31192446331859008, 0.3119240703560815, 0.31192739805711389, 0.311925199040617, 0.31192770334544195, 0.3119269756816932, 0.31192917769896106, 0.31193086289927191, 0.31192892969217334, 0.31192913994574978, 0.31193043469665355, 0.31193026906534643, 0.31193071548615214, 0.31193110346145969, 0.31193142946538222, 0.31192993646959527, 0.31193062816723965]\n"
     ]
    }
   ],
   "source": [
    "# This is extra code to answer the question in the PDF of lesson 7 doing 2 to 50 k fold ....\n",
    "\n",
    "kfolds = range(2,51,2)\n",
    "mean_mse = []\n",
    "mean_r2 = []\n",
    "for each_fold in kfolds:\n",
    "    kf = cross_validation.KFold(len(modeldata), n_folds=each_fold, shuffle=True)\n",
    "    mse_values = []\n",
    "    scores = []\n",
    "    n= 0\n",
    "    for train_index, test_index in kf:\n",
    "        lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "        mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "        scores.append(lm.score(modeldata, y))\n",
    "        n+=1\n",
    "    \n",
    "    mean_mse.append(np.mean(mse_values))\n",
    "    mean_r2.append(np.mean(scores))\n",
    "\n",
    "print mean_mse\n",
    "print mean_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "While the cross validated approach here generated more overall error, which of the two approaches would predict new data more accurately: the single model or the cross validated, averaged one? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are ways to improve our model with regularization. \n",
    "Let's check out the effects on MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ OLS ~~~\n",
      "OLS MSE:  1672.58110765\n",
      "OLS R2: 0.311934605989\n",
      "~~~ Lasso ~~~\n",
      "Lasso MSE:  1725.41581608\n",
      "Lasso R2: 0.290199495922\n",
      "~~~ Ridge ~~~\n",
      "Ridge MSE:  1672.60490113\n",
      "Ridge R2: 0.311924817843\n",
      "[ 112.50129738  -83.84805622  -13.38214934   -9.72671278  -10.46162477]\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(modeldata, y)\n",
    "\n",
    "# could also be on smae line as illustrate below\n",
    "print \"~~~ OLS ~~~\"\n",
    "print 'OLS MSE: ', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'OLS R2:', lm.score(modeldata, y)\n",
    "\n",
    "lm = linear_model.Lasso().fit(modeldata, y)\n",
    "print \"~~~ Lasso ~~~\"\n",
    "print 'Lasso MSE: ', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'Lasso R2:', lm.score(modeldata, y)\n",
    "# lasso throws away variables which makes the model worse (as results show) since the \n",
    "# model is already very simple\n",
    "\n",
    "lm = linear_model.Ridge().fit(modeldata, y)\n",
    "print \"~~~ Ridge ~~~\"\n",
    "print 'Ridge MSE: ', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'Ridge R2:', lm.score(modeldata, y)\n",
    "print lm.coef_\n",
    "# even ridge is slightly worse than regular OLS model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out the alphas can be done by \"hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1e-10\n",
      "[ 112.67353849  -83.99400867  -14.13184385  -10.45474561  -11.1736308 ]\n",
      "1672.60030785\n",
      "Alpha: 1e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 112.67353845  -83.99400863  -14.13183603  -10.45473779  -11.17362296]\n",
      "1672.60030788\n",
      "Alpha: 1e-08\n",
      "[ 112.67353812  -83.99400832  -14.13175774  -10.45465952  -11.17354466]\n",
      "1672.60030816\n",
      "Alpha: 1e-07\n",
      "[ 112.67353477  -83.99400521  -14.13097487  -10.4538768   -11.17276161]\n",
      "1672.60031101\n",
      "Alpha: 1e-06\n",
      "[ 112.67350128  -83.99397405  -14.12314614  -10.4460497   -11.16493108]\n",
      "1672.60033951\n",
      "Alpha: 1e-05\n",
      "[ 112.67316634  -83.9936625   -14.04485887  -10.36777861  -11.08662579]\n",
      "1672.60062569\n",
      "Alpha: 0.0001\n",
      "[ 112.66981758  -83.99054765  -13.26239352   -9.58547483  -10.30397994]\n",
      "1672.60360255\n",
      "Alpha: 0.001\n",
      "[ 112.63635975  -83.9594323    -5.45806601   -1.78275411   -2.49783247]\n",
      "1672.64489673\n",
      "Alpha: 0.01\n",
      "[ 112.39077334  -83.66325507   -2.90812399    0.66752444   -0.        ]\n",
      "1672.66932453\n",
      "Alpha: 0.1\n",
      "[ 109.93593998  -80.45002088   -1.82328187    0.77974702   -0.        ]\n",
      "1673.39565389\n",
      "Alpha: 1.0\n",
      "[ 86.81079432 -55.76414394   0.          -0.          -0.        ]\n",
      "1725.41581608\n",
      "Alpha: 10.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 100.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 1000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 10000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 100000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 1000000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 10000000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 100000000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 1000000000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n",
      "Alpha: 10000000000.0\n",
      "[ 0. -0.  0. -0. -0.]\n",
      "2430.84614081\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-10, 10, 21)\n",
    "# creates 21 observations between large range of -10 to 10 both to power of 10\n",
    "# a is the weight you give to the various variables and this for loop shows you\n",
    "# what different manually set weights do for the accuracy of your model\n",
    "\n",
    "for a in alphas:\n",
    "    print 'Alpha:', a\n",
    "    lm = linear_model.Lasso(alpha=a)\n",
    "    lm.fit(modeldata, y)\n",
    "    print lm.coef_\n",
    "    print metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "    \n",
    "    \n",
    "# pick the alpha with lowest mean squared error. Do note that in the above example we \n",
    "# used the same data for training and testing (modedata) which is obviously not good. WE\n",
    "# should have used a different data set for training than for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we can use grid search to make this faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': array([  1.00000e-10,   1.00000e-09,   1.00000e-08,   1.00000e-07,\n",
       "         1.00000e-06,   1.00000e-05,   1.00000e-04,   1.00000e-03,\n",
       "         1.00000e-02,   1.00000e-01,   1.00000e+00,   1.00000e+01,\n",
       "         1.00000e+02,   1.00000e+03,   1.00000e+04,   1.00000e+05,\n",
       "         1.00000e+06,   1.00000e+07,   1.00000e+08,   1.00000e+09,\n",
       "         1.00000e+10])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='mean_squared_error',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "\n",
    "alphas = np.logspace(-10, 10, 21)\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.Ridge(),\n",
    "    param_grid={'alpha': alphas},\n",
    "    scoring='mean_squared_error')\n",
    "\n",
    "gs.fit(modeldata, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1814.09369133\n"
     ]
    }
   ],
   "source": [
    "print gs.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mean squared error here comes in negative, so let's make it positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1814.09369133\n"
     ]
    }
   ],
   "source": [
    "print -gs.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explains which grid_search setup worked best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=10.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "print gs.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shows all the grid pairings and their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -1817.58711, std: 542.14315, params: {'alpha': 1e-10}, mean: -1817.58711, std: 542.14315, params: {'alpha': 1.0000000000000001e-09}, mean: -1817.58711, std: 542.14315, params: {'alpha': 1e-08}, mean: -1817.58711, std: 542.14315, params: {'alpha': 9.9999999999999995e-08}, mean: -1817.58711, std: 542.14315, params: {'alpha': 9.9999999999999995e-07}, mean: -1817.58711, std: 542.14317, params: {'alpha': 1.0000000000000001e-05}, mean: -1817.58707, std: 542.14331, params: {'alpha': 0.0001}, mean: -1817.58663, std: 542.14477, params: {'alpha': 0.001}, mean: -1817.58230, std: 542.15933, params: {'alpha': 0.01}, mean: -1817.54318, std: 542.30102, params: {'alpha': 0.10000000000000001}, mean: -1817.20111, std: 543.63587, params: {'alpha': 1.0}, mean: -1814.09369, std: 556.35563, params: {'alpha': 10.0}, mean: -1818.51694, std: 653.68607, params: {'alpha': 100.0}, mean: -2125.58777, std: 872.45270, params: {'alpha': 1000.0}, mean: -2458.08836, std: 951.30428, params: {'alpha': 10000.0}, mean: -2532.21151, std: 962.80083, params: {'alpha': 100000.0}, mean: -2541.38479, std: 963.98339, params: {'alpha': 1000000.0}, mean: -2542.32833, std: 964.10141, params: {'alpha': 10000000.0}, mean: -2542.42296, std: 964.11321, params: {'alpha': 100000000.0}, mean: -2542.43242, std: 964.11439, params: {'alpha': 1000000000.0}, mean: -2542.43337, std: 964.11450, params: {'alpha': 10000000000.0}]\n"
     ]
    }
   ],
   "source": [
    "print gs.grid_scores_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 is better than 6.2\n",
      "found better solution! using 5.2\n",
      "4.2 is better than 5.2\n",
      "found better solution! using 4.2\n",
      "3.2 is better than 4.2\n",
      "found better solution! using 3.2\n",
      "2.2 is better than 3.2\n",
      "found better solution! using 2.2\n",
      "1.2 is better than 2.2\n",
      "found better solution! using 1.2\n",
      "0.2 is better than 1.2\n",
      "found better solution! using 0.2\n",
      "6.0 is closest to 6.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_to_approach, start, steps, optimized = 6.2, 0., [-1, 1], False\n",
    "while not optimized:\n",
    "    current_distance = num_to_approach - start\n",
    "    got_better = False\n",
    "    next_steps = [start + i for i in steps]\n",
    "    for n in next_steps:\n",
    "        distance = np.abs(num_to_approach - n)\n",
    "        if distance < current_distance:\n",
    "            got_better = True\n",
    "            print distance, 'is better than', current_distance\n",
    "            current_distance = distance\n",
    "            start = n\n",
    "    if got_better:\n",
    "        print 'found better solution! using', current_distance\n",
    "        n += 1\n",
    "    else:\n",
    "        optimized = True\n",
    "        print start, 'is closest to', num_to_approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: \n",
    "implement a stopping point, similar to what n_iter would do in gradient descent when we've reached \"good enough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Application of Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent R2: 0.30853517891\n",
      "Gradient Descent MSE: 1680.84459185\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.SGDRegressor()\n",
    "lm.fit(modeldata, y)\n",
    "print \"Gradient Descent R2:\", lm.score(modeldata, y)\n",
    "print \"Gradient Descent MSE:\", metrics.mean_squared_error(y, lm.predict(modeldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: Untuned, how well did gradient descent perform compared to OLS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Practice: Bike data revisited\n",
    "\n",
    "There are tons of ways to approach a regression problem. The regularization techniques appended to ordinary least squares optimizes the size of coefficients to best account for error. Gradient Descent also introduces learning rate (how aggressively do we solve the problem), epsilon (at what point do we say the error margin is acceptable), and iterations (when should we stop no matter what?)\n",
    "\n",
    "For this deliverable, our goals are to:\n",
    "\n",
    "- implement the gradient descent approach to our bike-share modeling problem,\n",
    "- show how gradient descent solves and optimizes the solution,\n",
    "- demonstrate the grid_search module!\n",
    "\n",
    "While exploring the Gradient Descent regressor object, you'll build a grid search using the stochastic gradient descent estimator for the bike-share data set. Continue with either the model you evaluated last class or the simpler one from today. In particular, be sure to implement the \"param_grid\" in the grid search to get answers for the following questions:\n",
    "\n",
    "- With a set of alpha values between 10^-10 and 10^-1, how does the mean squared error change?\n",
    "- Based on the data, we know when to properly use l1 vs l2 regularization. By using a grid search with l1_ratios between 0 and 1 (increasing every 0.05), does that statement hold true? If not, did gradient descent have enough iterations?\n",
    "- How do these results change when you alter the learning rate (eta0)?\n",
    "\n",
    "**Bonus**: Can you see the advantages and disadvantages of using gradient descent after finishing this exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {} # put your gradient descent parameters here\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.SGDRegressor(),\n",
    "    cv=cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True),\n",
    "    param_grid=params,\n",
    "    scoring='mean_squared_error',\n",
    "    )\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "print 'BEST ESTIMATOR'\n",
    "print -gs.best_score_\n",
    "print gs.best_estimator_\n",
    "print 'ALL ESTIMATORS'\n",
    "print gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
